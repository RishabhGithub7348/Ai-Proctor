# AI Proctor - Real-time Interview Monitoring System

An intelligent AI-powered proctoring system that monitors interviews in real-time using dual-layer validation with edge ML models and cloud-based AI verification to minimize false positives while ensuring interview integrity.

## ðŸš€ What Makes This Different?

Unlike traditional proctoring systems that spam alerts with false positives, this system uses a **dual-layer validation approach**:

1. **Fast Edge Detection** (MediaPipe, YOLO) - Quickly identifies potential violations
2. **Smart AI Verification** (Google Gemini) - Verifies if violations are genuine before alerting

**Key Advantages:**
- ðŸŽ¯ **Dramatically Reduced False Positives**: AI distinguishes between genuine cheating vs. natural behavior
- ðŸ’° **Cost-Effective**: Only calls AI when creating alerts (not every frame)
- âš¡ **Performance Optimized**: SSIM frame skipping, black screen detection, smart processing
- ðŸ” **Explainable**: AI provides confidence scores and reasoning for each decision
- ðŸŽ“ **Production-Proven**: Uses confidence thresholds and techniques from real-world systems

## Features

### ðŸŽ¯ Core Detection Capabilities

- **Advanced Eye Gaze Tracking**:
  - Head pose estimation using OpenCV solvePnP with 3D-2D correspondence
  - Iris tracking with contour detection
  - Personalized calibration system (first 10 frames establish neutral position)
  - Detects looking away in 4 directions: left, right, up, down

- **Multiple Person Detection**:
  - Real-time face counting with MediaPipe Face Detection
  - Alerts when more than one person appears in video frame
  - Configurable face detection threshold

- **Intelligent Object Detection**:
  - YOLO v8 with COCO dataset (focused on critical objects)
  - Cell phone detection (class 67) - CRITICAL priority
  - Optional monitoring: laptops, books, keyboards
  - 0.6 confidence threshold (production-proven)

- **Audio Anomaly Detection**:
  - RMS energy and zero-crossing rate analysis
  - Multiple voice detection
  - Background conversation alerts

- **Tab/Window Switching Detection**:
  - Frontend-based Page Visibility API monitoring
  - Tracks when candidate switches tabs or minimizes window
  - Medium-severity alerts with timestamps

### ðŸ¤– AI-Powered Verification Layer (Gemini)

**Dual-Layer Validation System:**
1. **Layer 1 (Fast)**: Edge ML models (MediaPipe, YOLO) detect potential violations
2. **Layer 2 (Smart)**: When violations trigger alerts, screenshots are sent to Google Gemini AI for intelligent verification

**Benefits:**
- âœ… Dramatically reduces false positives (e.g., distinguishes phone from remote)
- âœ… Contextual understanding (brief thinking glances vs. reading from notes)
- âœ… Cost-effective (only calls API when creating alerts, not every frame)
- âœ… Explainable decisions with confidence scores and reasoning

**AI Verification Features:**
- Detailed prompt engineering for each violation type
- False positive detection (posters, mirrors, background objects)
- Severity assessment (low/medium/high/critical)
- Recommended actions (ignore/monitor/warn/flag)
- Confidence scoring (0.0-1.0) with 0.7 threshold

### ðŸš€ Performance Optimizations

- **Frame Analysis**: SSIM-based duplicate frame detection (skips >95% similar frames)
- **Black Screen Detection**: Alerts when camera is covered or off
- **Smart Frame Skipping**: Processes only relevant frames for efficiency
- **Alert Cooldown**: 3-second cooldown to prevent spam (configurable)

### ðŸ“Š Additional Features

- Real-time WebSocket communication for instant alerts
- Configurable alert thresholds and cooldown periods
- Session management with unique session IDs
- Alert severity levels (critical, high, medium, low)
- Live monitoring dashboard with statistics
- Tab switch counter and tracking
- AI verification metadata in alerts

## Tech Stack

### Backend
- **FastAPI**: High-performance Python web framework with async support
- **WebSocket**: Real-time bidirectional communication
- **MediaPipe**: Face detection (468 landmarks) and facial landmark tracking
- **OpenCV**: Image processing, computer vision, and solvePnP head pose estimation
- **YOLO v8**: State-of-the-art object detection (Ultralytics)
- **NumPy**: Numerical computations
- **scikit-image**: SSIM calculations for frame similarity
- **Google Gemini AI**: Multimodal AI for violation verification (gemini-1.5-flash)

### Frontend
- **Next.js 15**: React framework with App Router
- **TypeScript**: Type-safe JavaScript
- **TanStack Query (React Query)**: Server state management and data fetching
- **Axios**: HTTP client for REST API calls
- **Tailwind CSS**: Utility-first styling
- **WebRTC**: Browser media capture (getUserMedia API)
- **Page Visibility API**: Tab switching detection

### ML/AI Models
- **MediaPipe Face Mesh**: 468 facial landmarks for precise eye tracking
- **MediaPipe Face Detection**: Multiple person detection with confidence scoring
- **YOLOv8 Nano**: Fast object detection optimized for real-time inference
- **Google Gemini 1.5 Flash**: Multimodal AI for intelligent violation verification
- **OpenCV solvePnP**: 3D head pose estimation from 2D facial landmarks
- **Custom Audio Analysis**: RMS energy and zero-crossing rate (expandable to pyannote.audio)

## Project Structure

```
Ai-Proctor/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚   â”‚       â”œâ”€â”€ proctoring.py         # Proctoring API endpoints
â”‚   â”‚   â”‚       â””â”€â”€ session.py            # Session management
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â””â”€â”€ config.py                 # Configuration settings
â”‚   â”‚   â”œâ”€â”€ ml_models/
â”‚   â”‚   â”‚   â”œâ”€â”€ face_detector.py          # Face detection using MediaPipe
â”‚   â”‚   â”‚   â”œâ”€â”€ eye_tracker.py            # Eye gaze + head pose tracking
â”‚   â”‚   â”‚   â”œâ”€â”€ object_detector.py        # YOLO-based object detection
â”‚   â”‚   â”‚   â””â”€â”€ audio_analyzer.py         # Audio analysis
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ connection_manager.py     # WebSocket manager
â”‚   â”‚   â”‚   â”œâ”€â”€ proctoring_service.py     # Main proctoring logic
â”‚   â”‚   â”‚   â””â”€â”€ gemini_verifier.py        # ðŸ†• AI violation verification
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â””â”€â”€ frame_utils.py            # ðŸ†• Frame optimization (SSIM, black screen)
â”‚   â”‚   â””â”€â”€ main.py                       # FastAPI application
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ interview/
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx                  # Interview session page (ðŸ†• tab detection)
â”‚   â”‚   â”œâ”€â”€ dashboard/
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx                  # Dashboard page
â”‚   â”‚   â””â”€â”€ page.tsx                      # Home page
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ client.ts                 # Axios client
â”‚   â”‚   â”‚   â”œâ”€â”€ types.ts                  # TypeScript types
â”‚   â”‚   â”‚   â””â”€â”€ endpoints.ts              # API functions
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â”œâ”€â”€ useSession.ts             # Session hooks
â”‚   â”‚   â”‚   â”œâ”€â”€ useProctoring.ts          # Proctoring hooks
â”‚   â”‚   â”‚   â”œâ”€â”€ useWebSocket.ts           # WebSocket hook
â”‚   â”‚   â”‚   â””â”€â”€ index.ts                  # Hook exports
â”‚   â”‚   â””â”€â”€ providers/
â”‚   â”‚       â””â”€â”€ QueryProvider.tsx         # TanStack Query provider
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ .env.example
â”‚   â””â”€â”€ tailwind.config.ts
â”œâ”€â”€ README.md
â”œâ”€â”€ SETUP.md
â””â”€â”€ TANSTACK_QUERY_GUIDE.md
```

## Installation

### Prerequisites

- **Python 3.11** (recommended for MediaPipe compatibility on Apple Silicon)
- **Node.js 18+**
- **Webcam and microphone**
- **Modern browser** (Chrome, Firefox, Edge)
- **Google Gemini API Key** (for AI verification) - Get from [Google Cloud](https://console.cloud.google.com/getting-started)

### Backend Setup

1. Navigate to the backend directory:
```bash
cd backend
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Create a `.env` file and add your Gemini API key:
```bash
cp .env.example .env
# Edit .env and add:
# GEMINI_API_KEY=your-api-key-here
# GEMINI_MODEL=gemini-1.5-flash
# ENABLE_AI_VERIFICATION=false
# AI_VERIFICATION_CONFIDENCE_THRESHOLD=0.7
```

5. Run the backend server:
```bash
python -m uvicorn app.main:app --reload
```

The backend will be available at `http://localhost:8000`

### Backend Setup with Docker (Recommended for Windows)

If you're on Windows or having trouble setting up Python dependencies, use Docker:

1. **Install Docker Desktop**:
   - Download from [docker.com](https://www.docker.com/products/docker-desktop/)
   - Install and start Docker Desktop

2. **Build the Docker image**:
```bash
cd backend
docker build -t ai-proctor-backend .
```

3. **Run the container**:
```bash
# Basic run (without AI verification)
docker run -p 8000:8000 ai-proctor-backend

# With Gemini AI verification (optional)
docker run -p 8000:8000 \
  -e GEMINI_API_KEY=your-api-key-here \
  -e ENABLE_AI_VERIFICATION=true \
  ai-proctor-backend
```

4. **Verify it's running**:
   - Open http://localhost:8000 - Should show "AI Proctor API is running"
   - Open http://localhost:8000/health - Health check endpoint

**Useful Docker Commands:**
```bash
# Stop container
docker stop $(docker ps -q --filter ancestor=ai-proctor-backend)

# View logs
docker logs -f $(docker ps -q --filter ancestor=ai-proctor-backend)

# Rebuild after changes
docker build -t ai-proctor-backend . --no-cache
```

### Frontend Setup

1. Navigate to the frontend directory:
```bash
cd frontend
```

2. Install dependencies:
```bash
npm install
```

3. Run the development server:
```bash
npm run dev
```

The frontend will be available at `http://localhost:3000`

## Usage

1. **Start the Backend**: Run the FastAPI server
2. **Start the Frontend**: Run the Next.js development server
3. **Open Browser**: Navigate to `http://localhost:3000`
4. **Start Interview**: Click "Start Interview" button
5. **Grant Permissions**: Allow camera and microphone access
6. **Monitor**: The system will automatically detect violations

## Configuration

Edit `backend/.env` to customize:

```env
# Gemini AI Verification
GEMINI_API_KEY=your-gemini-api-key-here
GEMINI_MODEL=gemini-1.5-flash
ENABLE_AI_VERIFICATION=false
AI_VERIFICATION_CONFIDENCE_THRESHOLD=0.7

# ML Model Thresholds
MODEL_CONFIDENCE_THRESHOLD=0.6      # YOLO object detection (production: 0.6)
EYE_GAZE_THRESHOLD=2.0              # Seconds before eye tracking alert
ALERT_COOLDOWN_SECONDS=3            # Cooldown between alerts
MAX_FACES_ALLOWED=1                 # Maximum faces allowed in frame

# Recording (optional)
ENABLE_RECORDING=False
RECORDING_PATH=./recordings

# Server Configuration
HOST=0.0.0.0
PORT=8000
DEBUG=True
```

## API Endpoints

### REST API

- `GET /` - Health check
- `GET /health` - Detailed health status
- `POST /api/v1/session/create` - Create new session
- `GET /api/v1/session/{session_id}` - Get session details
- `POST /api/v1/session/{session_id}/end` - End session
- `GET /api/v1/proctoring/status/{session_id}` - Get proctoring status
- `GET /api/v1/proctoring/alerts/{session_id}` - Get session alerts

### WebSocket

- `WS /ws/{session_id}` - Real-time proctoring connection

## Detection Details

### Advanced Eye Gaze Tracking
**How it works:**
1. **Calibration Phase**: First 10 frames establish the user's neutral head position
2. **Head Pose Estimation**: Uses OpenCV solvePnP with 6 facial landmarks to calculate pitch, yaw, roll angles
3. **Iris Tracking**: Contour detection on thresholded eye regions for precise gaze direction
4. **Deviation Detection**: Compares current angles against calibrated neutral position
5. **Violation Trigger**: Alerts when looking away for > 2 seconds (configurable)

**Key Features:**
- MediaPipe Face Mesh with 468 landmarks
- Personalized calibration per user
- Detects: left, right, up, down, center
- Accounts for natural head movements

### Multiple Person Detection
**How it works:**
- MediaPipe Face Detection with confidence threshold
- Counts distinct faces in frame
- Configurable maximum allowed (default: 1)

**AI Verification checks:**
- Real people vs. posters/photos/screens
- Mirror reflections
- Pet faces (false positives)
- Background people vs. active helpers

### Intelligent Object Detection
**How it works:**
- YOLOv8 Nano (optimized for real-time)
- COCO dataset classes
- 0.6 confidence threshold (production-proven)
- Focused detection on critical objects

**Monitored Objects:**
- **CRITICAL**: Cell phone (class 67) - highest priority
- **Optional**: Laptop (63), Book (73), Keyboard (66), Mouse (64)

**AI Verification checks:**
- Phone vs. remote/calculator/glasses case
- Laptop vs. monitor/picture frame
- Active use vs. background items
- Distance from candidate

### Audio Analysis
- RMS energy calculation
- Zero-crossing rate analysis
- Baseline energy tracking
- Anomaly detection for multiple voices

### Tab/Window Switching
**How it works:**
- Page Visibility API monitors document.hidden state
- Detects tab switches, window minimization
- Creates medium-severity alerts with timestamps
- Counter displayed in dashboard stats

### Frame Optimization
**How it works:**
- **SSIM Analysis**: Skips frames with >95% similarity to previous frame
- **Black Screen Detection**: Alerts if average intensity < 30 (camera covered)
- **Smart Processing**: Only processes meaningful frames
- **Performance**: Reduces processing by ~60-70%

### AI Verification Layer (Gemini)
**How it works:**
1. ML model detects potential violation
2. If alert would be created (not in cooldown):
   - Capture frame screenshot
   - Send to Gemini with detailed context prompt
   - Gemini analyzes visual scene
3. Only create alert if Gemini confirms (confidence â‰¥ 0.7)

**Gemini Analysis:**
- Visual understanding of actual scene
- Context-aware decisions (thinking vs. cheating)
- False positive detection
- Explainable reasoning with confidence scores

## Development Roadmap

### âœ… Completed Features
- [x] Basic project structure
- [x] FastAPI backend with WebSocket
- [x] Next.js 15 frontend with video capture
- [x] TanStack Query integration
- [x] Face detection (MediaPipe)
- [x] Advanced eye gaze tracking with head pose estimation
- [x] Personalized calibration system
- [x] Object detection (YOLO v8)
- [x] Audio analysis
- [x] Alert system with severity levels
- [x] **AI-powered violation verification (Gemini)**
- [x] **Frame optimization (SSIM, black screen detection)**
- [x] **Tab/window switching detection**
- [x] **Detailed AI prompts for each violation type**
- [x] **Production-proven confidence thresholds**
- [x] Alert cooldown system
- [x] Live monitoring dashboard
- [x] Statistics and metrics display

### ðŸš§ In Progress / Future Features
- [ ] Violation history tracking for contextual AI decisions
- [ ] API call caching and budget optimization
- [ ] Database integration (PostgreSQL)
- [ ] Session recording with timestamps
- [ ] Post-interview violation report generation
- [ ] Advanced speaker diarization (pyannote.audio)
- [ ] Real-time dashboard with charts and analytics
- [ ] Hand movement/gesture detection (MediaPipe Hands)
- [ ] Customizable strictness levels per violation type
- [ ] Admin panel for configuration
- [ ] Multi-user support with role-based access
- [ ] Cloud deployment (AWS/GCP)
- [ ] Comprehensive test suite

## Performance Considerations

### Processing Speed
- **Frame capture**: 1 FPS (1 second interval, adjustable)
- **YOLOv8 Nano**: ~30ms inference on CPU, <10ms on GPU
- **MediaPipe Face Mesh**: ~15-20ms per frame
- **WebSocket latency**: <100ms for bidirectional communication
- **Gemini API**: ~1-3 seconds for AI verification (only called during alerts)

### Optimization Strategies
- **SSIM Frame Skipping**: Reduces processing by ~60-70% by skipping duplicate frames
- **Alert Cooldown**: 3-second cooldown prevents redundant API calls
- **Black Screen Detection**: Skips processing if camera is covered
- **Smart AI Verification**: Only calls Gemini when creating alerts (not every frame)
- **Frame Reduction**: Processes ~1 frame per second vs. 30 FPS video

### Resource Usage
- **CPU Usage**: 20-40% on modern processors (M1/M2 Mac, Intel i5+)
- **Memory**: ~500MB-1GB including ML models
- **Network**: ~100KB/s video upload, minimal for AI verification
- **Gemini API Cost**: ~$0.001 per verification (only when alerts triggered)

## Privacy & Security

- **Real-time Processing**: All video processing happens in real-time, no permanent storage by default
- **Optional Recording**: Session recording disabled by default, can be enabled if needed
- **Secure WebSocket**: WSS encryption for video stream transmission
- **Gemini API**: Only violation screenshots sent to Gemini during alerts (not continuous video)
- **No Third-Party Sharing**: Video data only processed by your server and optionally Gemini
- **GDPR Compliant**: With proper configuration and user consent
- **Configurable Privacy**: Can disable AI verification to keep all processing local

## Troubleshooting

### Camera not working
- Check browser permissions (allow camera/microphone access)
- Ensure no other app is using the camera
- Try a different browser (Chrome recommended)

### WebSocket connection failed
- Verify backend is running on port 8000
- Check CORS settings in `backend/app/core/config.py`
- Ensure firewall allows WebSocket connections
- Check browser console for connection errors

### Python/MediaPipe compatibility issues
- **Apple Silicon (M1/M2/M3/M4 Mac)**: Use Python 3.11 (`brew install python@3.11`)
- Ensure you're using the correct Python version: `python --version`
- Create fresh virtual environment with Python 3.11

### Gemini API errors
- Verify API key is correct in `.env` file
- Check API quota/billing at [Google AI Studio](https://makersuite.google.com/)
- Ensure `ENABLE_AI_VERIFICATION=true` if you want AI verification
- Can disable AI verification temporarily: `ENABLE_AI_VERIFICATION=false`

### Eye tracking not detecting properly
- Wait for calibration to complete (first 10 frames)
- Ensure good lighting on face
- Face should be clearly visible to camera
- Check logs for calibration completion message

### Too many false positives
- Increase `AI_VERIFICATION_CONFIDENCE_THRESHOLD` (e.g., 0.8)
- Adjust `MODEL_CONFIDENCE_THRESHOLD` for object detection
- Enable AI verification if not already enabled

### Slow performance
- Reduce frame rate in `interview/page.tsx` (increase interval)
- Use GPU acceleration for YOLO (requires CUDA setup)
- Lower video resolution in `getUserMedia` call
- Disable AI verification for faster processing (local-only)

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## License

MIT License - see LICENSE file for details

## Acknowledgments

- **Google MediaPipe** - Face detection and landmark tracking
- **Google Gemini AI** - Intelligent violation verification
- **Ultralytics YOLO** - Real-time object detection
- **FastAPI** - Modern Python web framework
- **Vercel Next.js** - React framework with excellent DX
- **TanStack Query** - Powerful data synchronization
- **Proctoring-AI** - Inspiration for production-proven techniques
- Open source community

## Support

For issues and questions:
- Create an issue on GitHub
- Check existing documentation
- Review configuration settings

---

**Note**: This is a development version. For production use, additional security measures, database integration, and thorough testing are required.
